# LLM Analysis Engine Configuration

# Model settings
model:
  # Primary model for analysis
  primary:
    # Model type (local, openai, anthropic, falcon, etc.)
    provider: "local"
    
    # Model name/identifier
    name: "phi-2-gguf"
    
    # Model path for local models
    path: "models/phi-2-instruct/"
    
    # GGUF model path (for GGUF models)
    gguf_model_path: "models/phi-2-instruct/phi-2.Q4_K_M.gguf"
    
    # Use GGUF implementation
    use_gguf: true
    
    # Maximum context length
    max_context_length: 2048
    
    # Temperature for generation
    temperature: 0.7
    
    # Top-p sampling
    top_p: 0.95
    
    # Max tokens to generate
    max_tokens: 1024
    
    # Force real model implementation 
    force_real: false
  
# Hardware acceleration for Jetson Orin Nano
hardware:
  # Enable hardware acceleration
  enable_acceleration: true
  
  # CUDA device ID (-1 for CPU only)
  cuda_device: 0
  
  # Enable TensorRT optimization
  use_tensorrt: true
  
  # Quantization level (4-bit, 8-bit, none)
  # Using 8-bit for better balance of performance and memory usage
  quantization: "8-bit"
  
  # CUDA stream configuration
  cuda_streams: 2
  
  # Memory limit in MB - increased for full GPU utilization
  memory_limit_mb: 8192

# Caching settings
caching:
  # Enable caching of analysis results
  enabled: true
  
  # Cache type (memory, redis, etc.)
  type: "memory"
  
  # Cache TTL in seconds
  ttl_seconds: 3600
  
  # Maximum cache size
  max_size_mb: 512

# Monitoring and logging
monitoring:
  # Log all prompts and completions
  log_prompts: false
  
  # Log token usage
  log_token_usage: true
  
  # Log latency metrics
  log_latency: true
  
  # Log path
  log_path: "logs/llm_analysis/"