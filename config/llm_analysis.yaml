# LLM Analysis Engine Configuration

# Model settings
model:
  # Primary model for analysis
  primary:
    # Model type (local, openai, anthropic, falcon, etc.)
    provider: "local"
    
    # Model name/identifier
    name: "llama3-8b-quantized"
    
    # Model path for local models
    path: "models/llama3-8b-q4/"
    
    # Maximum context length
    max_context_length: 4096
    
    # Temperature for generation
    temperature: 0.7
    
    # Top-p sampling
    top_p: 0.95
    
    # Max tokens to generate
    max_tokens: 1024
  
  # Fallback model (used if primary fails or for specific tasks)
  fallback:
    provider: "openai"
    name: "gpt-3.5-turbo"
    api_key: "${OPENAI_API_KEY}"  # Environment variable reference
    temperature: 0.7
    top_p: 0.95
    max_tokens: 1024

# Hardware acceleration for Jetson Orin Nano
hardware:
  # Enable hardware acceleration
  enable_acceleration: true
  
  # CUDA device ID (-1 for CPU only)
  cuda_device: 0
  
  # Enable TensorRT optimization
  use_tensorrt: true
  
  # Quantization level (4-bit, 8-bit)
  quantization: "4-bit"
  
  # CUDA stream configuration
  cuda_streams: 1
  
  # Memory limit in MB
  memory_limit_mb: 6144

# Recommendations settings
recommendations:
  # Enable recommendation generation
  enabled: true
  
  # Maximum number of recommendations per analysis
  max_recommendations: 5
  
  # Confidence threshold for inclusion
  confidence_threshold: 0.7
  
  # Ranking method (relevance, priority, etc.)
  ranking_method: "relevance"
  
  # Recommendation categories
  categories:
    - "compliance"
    - "customer_satisfaction"
    - "process_efficiency"
    - "upsell_opportunity"
    - "risk_mitigation"
  
  # Template path for recommendation prompts
  template_path: "templates/recommendations/"

# Compliance analysis settings
compliance:
  # Enable compliance analysis
  enabled: true
  
  # Regulatory frameworks to check
  frameworks:
    - name: "gdpr"
      enabled: true
    - name: "hipaa"
      enabled: false
    - name: "pci_dss"
      enabled: true
    - name: "tcpa"
      enabled: true
  
  # Path to compliance rule definitions
  rules_path: "config/compliance/rules/"
  
  # Minimum compliance score threshold for alerts
  alert_threshold: 0.8
  
  # Generate detailed compliance reports
  detailed_reports: true

# Policy QA settings
policy_qa:
  # Enable policy question answering
  enabled: true
  
  # Knowledge base paths for policies
  knowledge_base_paths:
    - "knowledge/policies/"
    - "knowledge/procedures/"
  
  # Retrieval method (bm25, embedding)
  retrieval_method: "embedding"
  
  # Number of documents to retrieve
  num_documents: 5
  
  # Document embedding model
  embedding_model: "all-MiniLM-L6-v2"

# Caching settings
caching:
  # Enable caching of analysis results
  enabled: true
  
  # Cache type (memory, redis, etc.)
  type: "memory"
  
  # Cache TTL in seconds
  ttl_seconds: 3600
  
  # Maximum cache size
  max_size_mb: 512

# Monitoring and logging
monitoring:
  # Log all prompts and completions
  log_prompts: false
  
  # Log token usage
  log_token_usage: true
  
  # Log latency metrics
  log_latency: true
  
  # Log path
  log_path: "logs/llm_analysis/"