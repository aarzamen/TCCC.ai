"""
Data Store module for TCCC.ai system.

This module implements the Data Store component responsible for persisting and 
managing all data generated by the TCCC.ai system.
"""

import os
import time
import json
import sqlite3
import datetime
import threading
import logging
import shutil
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
from contextlib import contextmanager

from tccc.utils.logging import get_logger
from tccc.utils.config import Config

logger = get_logger(__name__)


class DatabaseManager:
    """
    Manages SQLite database connections and operations.
    Implements connection pooling, WAL mode, and optimizations for Jetson hardware.
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize the database manager with provided configuration.
        
        Args:
            config: Configuration dictionary with database settings
        """
        self.config = config
        self.db_config = config.get('database', {})
        self.sqlite_config = self.db_config.get('sqlite', {})
        
        # Extract configuration values
        self.db_path = self.sqlite_config.get('database_path', 'data/tccc.db')
        self.wal_mode = self.sqlite_config.get('wal_mode', True)
        self.journal_mode = self.sqlite_config.get('journal_mode', 'wal')
        self.synchronous = self.sqlite_config.get('synchronous', 1)
        self.cache_size = self.sqlite_config.get('cache_size', 2000)
        self.page_size = self.sqlite_config.get('page_size', 4096)
        self.auto_vacuum = self.sqlite_config.get('auto_vacuum', 1)
        self.temp_store = self.sqlite_config.get('temp_store', 2)
        self.max_connections = self.sqlite_config.get('max_connections', 10)
        
        # Performance configuration
        self.perf_config = config.get('performance', {})
        self.jetson_config = self.perf_config.get('jetson', {})
        self.nvme_optimized = self.jetson_config.get('nvme_optimized', True)
        self.use_memory_mapping = self.jetson_config.get('use_memory_mapping', True)
        
        # Directory setup
        self.db_dir = os.path.dirname(self.db_path)
        if not os.path.exists(self.db_dir):
            os.makedirs(self.db_dir, exist_ok=True)
            
        # Connection pool
        self._connection_pool = []
        self._pool_lock = threading.RLock()
        
        # Initialize database
        self._initialize_database()
        
        # Vacuum and analyze scheduler
        self.vacuum_interval = self.perf_config.get('vacuum_interval_hours', 168)
        self.analyze_interval = self.perf_config.get('analyze_interval_hours', 24)
        
        if self.vacuum_interval > 0 or self.analyze_interval > 0:
            self._start_maintenance_scheduler()
    
    def _initialize_database(self):
        """Initialize the database with optimal settings."""
        conn = self._create_connection()
        try:
            # Set pragmas for optimization
            pragmas = [
                f"PRAGMA journal_mode={self.journal_mode}",
                f"PRAGMA synchronous={self.synchronous}",
                f"PRAGMA cache_size={self.cache_size}",
                f"PRAGMA page_size={self.page_size}",
                f"PRAGMA auto_vacuum={self.auto_vacuum}",
                f"PRAGMA temp_store={self.temp_store}",
            ]
            
            # NVMe specific optimizations
            if self.nvme_optimized:
                pragmas.extend([
                    "PRAGMA locking_mode=EXCLUSIVE",
                    "PRAGMA mmap_size=268435456",  # 256MB memory mapping
                ])
                
            # Memory mapping if enabled
            if self.use_memory_mapping:
                memory_limit = self.jetson_config.get('memory_limit_mb', 512)
                pragmas.append(f"PRAGMA mmap_size={memory_limit * 1024 * 1024}")
                
            # Execute pragmas
            cursor = conn.cursor()
            for pragma in pragmas:
                cursor.execute(pragma)
                
            # Create tables if they don't exist
            self._create_tables(conn)
            
            conn.commit()
        except Exception as e:
            logger.error(f"Error initializing database: {e}")
            raise
        finally:
            self._return_connection(conn)
    
    def _create_tables(self, conn):
        """
        Create database tables if they don't exist.
        
        Args:
            conn: Database connection
        """
        cursor = conn.cursor()
        
        # Events table - stores medical events
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS events (
            event_id TEXT PRIMARY KEY,
            timestamp TEXT NOT NULL,
            type TEXT NOT NULL,
            source TEXT NOT NULL,
            severity TEXT,
            patient_id TEXT,
            session_id TEXT,
            data TEXT NOT NULL,
            metadata TEXT,
            created_at TEXT NOT NULL,
            updated_at TEXT
        )
        ''')
        
        # Create indexes for events table
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_events_timestamp ON events(timestamp)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_events_type ON events(type)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_events_patient ON events(patient_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_events_session ON events(session_id)')
        
        # Reports table - stores generated reports
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS reports (
            report_id TEXT PRIMARY KEY,
            timestamp TEXT NOT NULL,
            type TEXT NOT NULL,
            title TEXT,
            patient_id TEXT,
            session_id TEXT,
            content TEXT NOT NULL,
            metadata TEXT,
            created_at TEXT NOT NULL,
            updated_at TEXT
        )
        ''')
        
        # Create indexes for reports table
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_reports_timestamp ON reports(timestamp)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_reports_type ON reports(type)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_reports_patient ON reports(patient_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_reports_session ON reports(session_id)')
        
        # Sessions table - stores session metadata
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS sessions (
            session_id TEXT PRIMARY KEY,
            start_time TEXT NOT NULL,
            end_time TEXT,
            location TEXT,
            patient_id TEXT,
            provider_id TEXT,
            status TEXT NOT NULL,
            metadata TEXT,
            created_at TEXT NOT NULL,
            updated_at TEXT
        )
        ''')
        
        # Create indexes for sessions table
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_sessions_time ON sessions(start_time, end_time)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_sessions_patient ON sessions(patient_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_sessions_provider ON sessions(provider_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_sessions_status ON sessions(status)')
        
        # Metrics table - stores system metrics
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS metrics (
            metric_id TEXT PRIMARY KEY,
            timestamp TEXT NOT NULL,
            category TEXT NOT NULL,
            name TEXT NOT NULL,
            value REAL NOT NULL,
            unit TEXT,
            metadata TEXT,
            created_at TEXT NOT NULL
        )
        ''')
        
        # Create indexes for metrics table
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_metrics_timestamp ON metrics(timestamp)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_metrics_category ON metrics(category)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_metrics_name ON metrics(name)')
        
        # Backups table - tracks backup history
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS backups (
            backup_id TEXT PRIMARY KEY,
            timestamp TEXT NOT NULL,
            path TEXT NOT NULL,
            size INTEGER NOT NULL,
            checksum TEXT NOT NULL,
            status TEXT NOT NULL,
            metadata TEXT,
            created_at TEXT NOT NULL
        )
        ''')
        
        # Create index for backups table
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_backups_timestamp ON backups(timestamp)')
        
        conn.commit()
    
    def _create_connection(self):
        """
        Create a new SQLite connection.
        
        Returns:
            SQLite connection object
        """
        conn = sqlite3.connect(self.db_path, timeout=30.0, isolation_level=None)
        conn.row_factory = sqlite3.Row
        return conn
    
    def _get_connection(self):
        """
        Get a connection from the pool or create a new one.
        
        Returns:
            SQLite connection object
        """
        with self._pool_lock:
            if self._connection_pool:
                return self._connection_pool.pop()
            else:
                return self._create_connection()
    
    def _return_connection(self, conn):
        """
        Return a connection to the pool or close it if pool is full.
        
        Args:
            conn: SQLite connection object
        """
        with self._pool_lock:
            if len(self._connection_pool) < self.max_connections:
                self._connection_pool.append(conn)
            else:
                conn.close()
    
    @contextmanager
    def get_connection(self):
        """
        Context manager for database connections.
        
        Yields:
            SQLite connection object
        """
        conn = self._get_connection()
        try:
            yield conn
        finally:
            self._return_connection(conn)
    
    @contextmanager
    def transaction(self):
        """
        Context manager for database transactions.
        
        Yields:
            SQLite connection object with active transaction
        """
        conn = self._get_connection()
        try:
            conn.execute("BEGIN")
            yield conn
            conn.execute("COMMIT")
        except Exception as e:
            conn.execute("ROLLBACK")
            logger.error(f"Transaction failed: {e}")
            raise
        finally:
            self._return_connection(conn)
    
    def close_all_connections(self):
        """Close all connections in the pool."""
        with self._pool_lock:
            for conn in self._connection_pool:
                conn.close()
            self._connection_pool.clear()
    
    def _start_maintenance_scheduler(self):
        """Start the database maintenance scheduler."""
        
        def maintenance_task():
            """Background task for database maintenance."""
            last_vacuum_time = 0
            last_analyze_time = 0
            
            while True:
                current_time = time.time()
                
                # Check if vacuum is needed
                if self.vacuum_interval > 0 and current_time - last_vacuum_time > self.vacuum_interval * 3600:
                    self._vacuum_database()
                    last_vacuum_time = current_time
                
                # Check if analyze is needed
                if self.analyze_interval > 0 and current_time - last_analyze_time > self.analyze_interval * 3600:
                    self._analyze_database()
                    last_analyze_time = current_time
                
                # Sleep for an hour
                time.sleep(3600)
        
        # Start maintenance thread
        maintenance_thread = threading.Thread(
            target=maintenance_task,
            name="DataStore-Maintenance",
            daemon=True
        )
        maintenance_thread.start()
    
    def _vacuum_database(self):
        """Run VACUUM on the database to optimize storage."""
        try:
            with self.get_connection() as conn:
                conn.execute("VACUUM")
            logger.info("Database VACUUM completed successfully")
        except Exception as e:
            logger.error(f"Database VACUUM failed: {e}")
    
    def _analyze_database(self):
        """Run ANALYZE on the database to optimize query planning."""
        try:
            with self.get_connection() as conn:
                conn.execute("ANALYZE")
            logger.info("Database ANALYZE completed successfully")
        except Exception as e:
            logger.error(f"Database ANALYZE failed: {e}")
    
    def get_database_stats(self):
        """
        Get database statistics.
        
        Returns:
            Dictionary with database statistics
        """
        stats = {}
        try:
            with self.get_connection() as conn:
                cursor = conn.cursor()
                
                # Get database size
                cursor.execute("PRAGMA page_count")
                page_count = cursor.fetchone()[0]
                
                cursor.execute("PRAGMA page_size")
                page_size = cursor.fetchone()[0]
                
                stats['size_bytes'] = page_count * page_size
                stats['size_mb'] = stats['size_bytes'] / (1024 * 1024)
                
                # Get table counts
                tables = ['events', 'reports', 'sessions', 'metrics', 'backups']
                for table in tables:
                    cursor.execute(f"SELECT COUNT(*) FROM {table}")
                    stats[f'{table}_count'] = cursor.fetchone()[0]
                
                # Get database info
                cursor.execute("PRAGMA journal_mode")
                stats['journal_mode'] = cursor.fetchone()[0]
                
                cursor.execute("PRAGMA synchronous")
                stats['synchronous'] = cursor.fetchone()[0]
                
            return stats
        except Exception as e:
            logger.error(f"Error getting database stats: {e}")
            return {"error": str(e)}


class BackupManager:
    """
    Manages database backups and recovery.
    """
    
    def __init__(self, db_manager: DatabaseManager, config: Dict[str, Any]):
        """
        Initialize the backup manager.
        
        Args:
            db_manager: DatabaseManager instance
            config: Configuration dictionary
        """
        self.db_manager = db_manager
        self.config = config
        
        # Extract configuration
        self.backup_config = config.get('backup', {})
        self.enabled = self.backup_config.get('enabled', True)
        self.directory = self.backup_config.get('directory', 'data/backups')
        self.interval_hours = self.backup_config.get('interval_hours', 24)
        self.max_backups = self.backup_config.get('max_backups', 7)
        self.compression_level = self.backup_config.get('compression_level', 6)
        
        # Create backup directory if it doesn't exist
        if not os.path.exists(self.directory):
            os.makedirs(self.directory, exist_ok=True)
        
        # Start backup scheduler if enabled
        if self.enabled and self.interval_hours > 0:
            self._start_backup_scheduler()
    
    def _start_backup_scheduler(self):
        """Start the backup scheduler."""
        
        def backup_task():
            """Background task for scheduled backups."""
            while True:
                try:
                    self.create_backup()
                    # Cleanup old backups
                    self._cleanup_old_backups()
                except Exception as e:
                    logger.error(f"Scheduled backup failed: {e}")
                
                # Sleep until next backup
                time.sleep(self.interval_hours * 3600)
        
        # Start backup thread
        backup_thread = threading.Thread(
            target=backup_task,
            name="DataStore-Backup",
            daemon=True
        )
        backup_thread.start()
    
    def create_backup(self, label: str = None) -> Dict[str, Any]:
        """
        Create a database backup.
        
        Args:
            label: Optional label for the backup
            
        Returns:
            Dictionary with backup details
        """
        try:
            # Generate backup filename
            timestamp = datetime.datetime.now().isoformat(timespec='seconds')
            backup_id = f"backup_{int(time.time())}"
            if label:
                backup_id = f"{backup_id}_{label}"
            
            backup_filename = f"{backup_id}.db"
            backup_path = os.path.join(self.directory, backup_filename)
            
            # Ensure the database is in a consistent state
            with self.db_manager.get_connection() as conn:
                conn.execute("PRAGMA wal_checkpoint(FULL)")
            
            # Create backup
            source_path = self.db_manager.db_path
            shutil.copy2(source_path, backup_path)
            
            # Calculate size and checksum
            size = os.path.getsize(backup_path)
            
            # Calculate SHA-256 checksum
            sha256_hash = hashlib.sha256()
            with open(backup_path, "rb") as f:
                # Read in chunks for large files
                for byte_block in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(byte_block)
            checksum = sha256_hash.hexdigest()
            
            # Create backup record
            backup_record = {
                "backup_id": backup_id,
                "timestamp": timestamp,
                "path": backup_path,
                "size": size,
                "checksum": checksum,
                "status": "completed",
                "metadata": json.dumps({"label": label} if label else {}),
                "created_at": timestamp
            }
            
            # Store backup record in database
            with self.db_manager.transaction() as conn:
                conn.execute(
                    """
                    INSERT INTO backups 
                    (backup_id, timestamp, path, size, checksum, status, metadata, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                    (
                        backup_record["backup_id"],
                        backup_record["timestamp"],
                        backup_record["path"],
                        backup_record["size"],
                        backup_record["checksum"],
                        backup_record["status"],
                        backup_record["metadata"],
                        backup_record["created_at"]
                    )
                )
            
            logger.info(f"Database backup created: {backup_path} ({size} bytes)")
            return backup_record
            
        except Exception as e:
            logger.error(f"Backup failed: {e}")
            raise
    
    def restore_backup(self, backup_id: str) -> bool:
        """
        Restore database from backup.
        
        Args:
            backup_id: ID of the backup to restore
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Get backup details from database
            with self.db_manager.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT * FROM backups WHERE backup_id = ?", (backup_id,))
                backup = cursor.fetchone()
                
            if not backup:
                logger.error(f"Backup not found: {backup_id}")
                return False
            
            # Convert to dictionary
            backup_dict = dict(backup)
            
            # Verify backup file exists
            backup_path = backup_dict["path"]
            if not os.path.exists(backup_path):
                logger.error(f"Backup file not found: {backup_path}")
                return False
            
            # Verify checksum
            sha256_hash = hashlib.sha256()
            with open(backup_path, "rb") as f:
                for byte_block in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(byte_block)
            calculated_checksum = sha256_hash.hexdigest()
            
            if calculated_checksum != backup_dict["checksum"]:
                logger.error(f"Backup checksum mismatch: {backup_id}")
                return False
            
            # Close all database connections
            self.db_manager.close_all_connections()
            
            # Create a backup of the current database before restoring
            current_db_path = self.db_manager.db_path
            timestamp = int(time.time())
            pre_restore_backup = f"{current_db_path}.{timestamp}.pre_restore"
            shutil.copy2(current_db_path, pre_restore_backup)
            
            # Restore backup
            shutil.copy2(backup_path, current_db_path)
            
            logger.info(f"Database restored from backup: {backup_id}")
            return True
            
        except Exception as e:
            logger.error(f"Restore failed: {e}")
            raise
    
    def list_backups(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        List available backups.
        
        Args:
            limit: Maximum number of backups to return
            
        Returns:
            List of backup dictionaries
        """
        try:
            with self.db_manager.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(
                    "SELECT * FROM backups ORDER BY timestamp DESC LIMIT ?", 
                    (limit,)
                )
                backups = [dict(row) for row in cursor.fetchall()]
            
            return backups
        except Exception as e:
            logger.error(f"Error listing backups: {e}")
            return []
    
    def _cleanup_old_backups(self):
        """Remove old backups based on max_backups setting."""
        try:
            if self.max_backups <= 0:
                return
            
            with self.db_manager.get_connection() as conn:
                cursor = conn.cursor()
                # Get list of backups ordered by timestamp
                cursor.execute("SELECT backup_id, path FROM backups ORDER BY timestamp DESC")
                backups = cursor.fetchall()
                
                # Keep only max_backups number of backups
                if len(backups) <= self.max_backups:
                    return
                
                backups_to_delete = backups[self.max_backups:]
                
                for backup in backups_to_delete:
                    backup_id = backup[0]
                    path = backup[1]
                    
                    # Delete file if it exists
                    if os.path.exists(path):
                        os.remove(path)
                    
                    # Remove from database
                    conn.execute("DELETE FROM backups WHERE backup_id = ?", (backup_id,))
                
                logger.info(f"Cleaned up {len(backups_to_delete)} old backups")
                
        except Exception as e:
            logger.error(f"Error cleaning up old backups: {e}")


class DataStore:
    """
    Implementation of the DataStoreInterface.
    Manages persistence of events, reports, and provides query capabilities.
    """
    
    def __init__(self):
        """Initialize the data store."""
        self.initialized = False
        self.config = None
        self.db_manager = None
        self.backup_manager = None
        
        # Settings from config
        self.query_cache_enabled = False
        self.query_cache_ttl = 300
        self.query_cache = {}
        self.query_timestamp = {}
        
        # Context generation settings
        self.context_max_tokens = 2000
        self.context_time_window = 300  # seconds
        
        # Lock for thread safety
        self.lock = threading.RLock()
    
    def initialize(self, config: Dict[str, Any]) -> bool:
        """
        Initialize the data store with configuration.
        
        Args:
            config: Configuration dictionary
            
        Returns:
            True if initialization succeeded, False otherwise
        """
        try:
            with self.lock:
                self.config = config
                
                # Initialize database manager
                self.db_manager = DatabaseManager(config)
                
                # Initialize backup manager
                self.backup_manager = BackupManager(self.db_manager, config)
                
                # Extract query settings
                query_config = config.get('query', {})
                self.query_cache_enabled = query_config.get('cache_enabled', False)
                self.query_cache_ttl = query_config.get('cache_ttl', 300)
                
                # Extract context settings
                context_config = config.get('context', {})
                self.context_max_tokens = context_config.get('max_tokens', 2000)
                self.context_time_window = context_config.get('time_window', 300)
                
                self.initialized = True
                logger.info("Data Store initialized successfully")
                return True
                
        except Exception as e:
            logger.error(f"Failed to initialize Data Store: {e}")
            return False
    
    def store_event(self, event: Dict[str, Any]) -> str:
        """
        Store medical event and return event ID.
        
        Args:
            event: Event data dictionary
            
        Returns:
            Event ID
        """
        if not self.initialized:
            raise RuntimeError("Data Store is not initialized")
        
        try:
            # Generate unique ID if not provided
            event_id = event.get('event_id', f"evt_{int(time.time())}_{hash(str(event))}")
            
            # Set timestamps if not provided
            timestamp = event.get('timestamp', datetime.datetime.now().isoformat())
            created_at = datetime.datetime.now().isoformat()
            
            # Extract event fields
            event_type = event.get('type', 'unknown')
            source = event.get('source', 'system')
            severity = event.get('severity')
            patient_id = event.get('patient_id')
            session_id = event.get('session_id')
            
            # Convert data and metadata to JSON
            data = json.dumps(event.get('data', {}))
            metadata = json.dumps(event.get('metadata', {}))
            
            # Insert event into database
            with self.db_manager.transaction() as conn:
                conn.execute(
                    """
                    INSERT INTO events
                    (event_id, timestamp, type, source, severity, patient_id, session_id,
                    data, metadata, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, NULL)
                    """,
                    (
                        event_id, timestamp, event_type, source, severity, patient_id, session_id,
                        data, metadata, created_at
                    )
                )
                
            logger.debug(f"Stored event: {event_id}")
            return event_id
            
        except Exception as e:
            logger.error(f"Failed to store event: {e}")
            raise
    
    def store_report(self, report: Dict[str, Any]) -> str:
        """
        Store generated report and return report ID.
        
        Args:
            report: Report data dictionary
            
        Returns:
            Report ID
        """
        if not self.initialized:
            raise RuntimeError("Data Store is not initialized")
        
        try:
            # Generate unique ID if not provided
            report_id = report.get('report_id', f"rep_{int(time.time())}_{hash(str(report))}")
            
            # Set timestamps if not provided
            timestamp = report.get('timestamp', datetime.datetime.now().isoformat())
            created_at = datetime.datetime.now().isoformat()
            
            # Extract report fields
            report_type = report.get('type', 'unknown')
            title = report.get('title')
            patient_id = report.get('patient_id')
            session_id = report.get('session_id')
            
            # Convert content and metadata to JSON if they're not strings
            content = report.get('content', '')
            if not isinstance(content, str):
                content = json.dumps(content)
                
            metadata = json.dumps(report.get('metadata', {}))
            
            # Insert report into database
            with self.db_manager.transaction() as conn:
                conn.execute(
                    """
                    INSERT INTO reports
                    (report_id, timestamp, type, title, patient_id, session_id,
                    content, metadata, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, NULL)
                    """,
                    (
                        report_id, timestamp, report_type, title, patient_id, session_id,
                        content, metadata, created_at
                    )
                )
                
            logger.debug(f"Stored report: {report_id}")
            return report_id
            
        except Exception as e:
            logger.error(f"Failed to store report: {e}")
            raise
    
    def query_events(self, filters: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Query events based on filters.
        
        Args:
            filters: Query filters (type, time range, patient, etc.)
            
        Returns:
            List of matching events
        """
        if not self.initialized:
            raise RuntimeError("Data Store is not initialized")
        
        try:
            # Check cache if enabled
            if self.query_cache_enabled:
                cache_key = f"events_{hash(str(filters))}"
                if cache_key in self.query_cache:
                    # Check if cache is still valid
                    if time.time() - self.query_timestamp.get(cache_key, 0) < self.query_cache_ttl:
                        return self.query_cache[cache_key]
            
            # Build query
            query = "SELECT * FROM events WHERE 1=1"
            params = []
            
            # Apply filters
            if 'type' in filters:
                query += " AND type = ?"
                params.append(filters['type'])
                
            if 'source' in filters:
                query += " AND source = ?"
                params.append(filters['source'])
                
            if 'severity' in filters:
                query += " AND severity = ?"
                params.append(filters['severity'])
                
            if 'patient_id' in filters:
                query += " AND patient_id = ?"
                params.append(filters['patient_id'])
                
            if 'session_id' in filters:
                query += " AND session_id = ?"
                params.append(filters['session_id'])
                
            if 'start_time' in filters:
                query += " AND timestamp >= ?"
                params.append(filters['start_time'])
                
            if 'end_time' in filters:
                query += " AND timestamp <= ?"
                params.append(filters['end_time'])
                
            # Order by timestamp
            query += " ORDER BY timestamp DESC"
            
            # Apply limit if specified
            if 'limit' in filters:
                query += " LIMIT ?"
                params.append(int(filters['limit']))
            
            # Execute query
            with self.db_manager.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(query, params)
                rows = cursor.fetchall()
                
                # Convert rows to dictionaries and parse JSON fields
                events = []
                for row in rows:
                    event = dict(row)
                    event['data'] = json.loads(event['data'])
                    event['metadata'] = json.loads(event['metadata'])
                    events.append(event)
                
            # Cache results if enabled
            if self.query_cache_enabled:
                self.query_cache[cache_key] = events
                self.query_timestamp[cache_key] = time.time()
                
                # Clean up old cache entries
                self._cleanup_cache()
                
            return events
            
        except Exception as e:
            logger.error(f"Failed to query events: {e}")
            raise
    
    def get_timeline(self, start_time: str, end_time: str) -> List[Dict[str, Any]]:
        """
        Get timeline of events within time range.
        
        Args:
            start_time: Start time in ISO format
            end_time: End time in ISO format
            
        Returns:
            List of events in chronological order
        """
        if not self.initialized:
            raise RuntimeError("Data Store is not initialized")
        
        try:
            # Query events within time range
            return self.query_events({
                'start_time': start_time,
                'end_time': end_time
            })
            
        except Exception as e:
            logger.error(f"Failed to get timeline: {e}")
            raise
    
    def get_context(self, current_time: str, window_seconds: int = 300) -> Dict[str, Any]:
        """
        Get historical context for current processing.
        
        Args:
            current_time: Current time in ISO format
            window_seconds: Time window in seconds (default: 300)
            
        Returns:
            Context dictionary with events and reports
        """
        if not self.initialized:
            raise RuntimeError("Data Store is not initialized")
        
        try:
            # Parse current time
            if isinstance(current_time, str):
                dt = datetime.datetime.fromisoformat(current_time)
            else:
                dt = current_time
                
            # Calculate start time
            start_time = (dt - datetime.timedelta(seconds=window_seconds)).isoformat()
            
            # Query events within time window
            with self.db_manager.get_connection() as conn:
                cursor = conn.cursor()
                
                # Get events
                cursor.execute(
                    """
                    SELECT * FROM events 
                    WHERE timestamp >= ? AND timestamp <= ?
                    ORDER BY timestamp DESC
                    """,
                    (start_time, dt.isoformat())
                )
                events = []
                for row in cursor.fetchall():
                    event = dict(row)
                    event['data'] = json.loads(event['data'])
                    event['metadata'] = json.loads(event['metadata'])
                    events.append(event)
                
                # Get reports
                cursor.execute(
                    """
                    SELECT * FROM reports 
                    WHERE timestamp >= ? AND timestamp <= ?
                    ORDER BY timestamp DESC
                    """,
                    (start_time, dt.isoformat())
                )
                reports = []
                for row in cursor.fetchall():
                    report = dict(row)
                    if isinstance(report['content'], str) and report['content'].startswith('{'):
                        try:
                            report['content'] = json.loads(report['content'])
                        except:
                            pass  # Keep as string if not valid JSON
                    report['metadata'] = json.loads(report['metadata'])
                    reports.append(report)
                
                # Get active sessions
                cursor.execute(
                    """
                    SELECT * FROM sessions 
                    WHERE start_time <= ? AND (end_time IS NULL OR end_time >= ?)
                    """,
                    (dt.isoformat(), start_time)
                )
                sessions = []
                for row in cursor.fetchall():
                    session = dict(row)
                    session['metadata'] = json.loads(session['metadata'])
                    sessions.append(session)
            
            # Create context
            context = {
                'timestamp': dt.isoformat(),
                'time_window_seconds': window_seconds,
                'events': events,
                'reports': reports,
                'sessions': sessions
            }
            
            # Limit context size based on config
            context_config = self.config.get('context', {})
            include_metadata = context_config.get('include_metadata', True)
            order_by = context_config.get('order_by', 'time')
            
            # Sort events by timestamp or relevance
            if order_by == 'time':
                context['events'] = sorted(
                    context['events'], 
                    key=lambda x: x['timestamp'], 
                    reverse=True
                )
            
            # Remove metadata if not needed to reduce size
            if not include_metadata:
                for event in context['events']:
                    del event['metadata']
                for report in context['reports']:
                    del report['metadata']
                for session in context['sessions']:
                    del session['metadata']
            
            return context
            
        except Exception as e:
            logger.error(f"Failed to get context: {e}")
            raise
    
    def get_status(self) -> Dict[str, Any]:
        """
        Return current status of the data store.
        
        Returns:
            Status dictionary
        """
        if not self.initialized:
            return {'status': 'not_initialized'}
        
        try:
            # Get database stats
            db_stats = self.db_manager.get_database_stats()
            
            # Get backup info
            backups = self.backup_manager.list_backups(limit=1)
            last_backup = backups[0] if backups else None
            
            # Build status info
            status = {
                'status': 'active',
                'initialized': self.initialized,
                'database': {
                    'path': self.db_manager.db_path,
                    'size_mb': db_stats.get('size_mb', 0),
                    'journal_mode': db_stats.get('journal_mode', 'unknown'),
                    'counts': {
                        'events': db_stats.get('events_count', 0),
                        'reports': db_stats.get('reports_count', 0),
                        'sessions': db_stats.get('sessions_count', 0),
                        'metrics': db_stats.get('metrics_count', 0),
                        'backups': db_stats.get('backups_count', 0)
                    }
                },
                'backup': {
                    'enabled': self.backup_manager.enabled,
                    'directory': self.backup_manager.directory,
                    'last_backup': last_backup['timestamp'] if last_backup else None
                },
                'cache': {
                    'enabled': self.query_cache_enabled,
                    'ttl_seconds': self.query_cache_ttl,
                    'entries': len(self.query_cache) if hasattr(self, 'query_cache') else 0
                }
            }
            
            return status
            
        except Exception as e:
            logger.error(f"Failed to get status: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _cleanup_cache(self):
        """Clean up expired cache entries."""
        if not self.query_cache_enabled:
            return
        
        current_time = time.time()
        expired_keys = []
        
        for key, timestamp in self.query_timestamp.items():
            if current_time - timestamp > self.query_cache_ttl:
                expired_keys.append(key)
        
        for key in expired_keys:
            if key in self.query_cache:
                del self.query_cache[key]
            if key in self.query_timestamp:
                del self.query_timestamp[key]
                
    def backup(self, label: str = None) -> Dict[str, Any]:
        """
        Create a database backup.
        
        Args:
            label: Optional label for the backup
            
        Returns:
            Backup details
        """
        if not self.initialized:
            raise RuntimeError("Data Store is not initialized")
        
        return self.backup_manager.create_backup(label)
    
    def restore(self, backup_id: str) -> bool:
        """
        Restore from a backup.
        
        Args:
            backup_id: ID of the backup to restore
            
        Returns:
            True if successful, False otherwise
        """
        if not self.initialized:
            raise RuntimeError("Data Store is not initialized")
        
        return self.backup_manager.restore_backup(backup_id)
    
    def migrate_schema(self, new_schema_version: str) -> bool:
        """
        Migrate database schema to a new version.
        
        Args:
            new_schema_version: New schema version to migrate to
            
        Returns:
            True if successful, False otherwise
        """
        # Not implemented - would include version-specific migration scripts
        raise NotImplementedError("Schema migration not implemented")
        
    def shutdown(self) -> bool:
        """
        Shutdown the data store, releasing resources.
        
        Returns:
            True if successful, False otherwise
        """
        if not self.initialized:
            return True
            
        try:
            # Close all database connections
            if self.db_manager:
                self.db_manager.close_all_connections()
                
            # Clear caches
            self.query_cache.clear()
            self.query_timestamp.clear()
            
            self.initialized = False
            logger.info("DataStore shutdown complete")
            return True
            
        except Exception as e:
            logger.error(f"Error shutting down DataStore: {e}")
            return False